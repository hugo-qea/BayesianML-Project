{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Machine Learning\n",
    "## Challenges in Markov chain Monte Carlo for Bayesian neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('../source/')\n",
    "from utils import effective_sample_size\n",
    "from HMC import HMC\n",
    "from MALA import MALA, AdaptiveMALA\n",
    "from RWHM import Metropolis_Hastings\n",
    "from model import Model\n",
    "from utils import effective_sample_size, gelman_rubin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_target_factory(\n",
    "        mlp: Model,\n",
    "        X: Union[np.ndarray, torch.Tensor],\n",
    "        y: Union[np.ndarray, torch.Tensor],\n",
    "    ):\n",
    "\n",
    "    def log_target_fn(theta):\n",
    "        log_target_tensor = mlp.compute_log_target(X, y, theta)\n",
    "        log_target_array = log_target_tensor.cpu().detach().numpy()\n",
    "        return log_target_array\n",
    "        \n",
    "    return log_target_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_log_target_factory(\n",
    "        mlp: Model,\n",
    "        X: Union[np.ndarray, torch.Tensor],\n",
    "        y: Union[np.ndarray, torch.Tensor],\n",
    "    ):\n",
    "\n",
    "    def grad_log_target_fn(theta):\n",
    "        log_target_tensor = mlp.compute_log_target(X, y, theta)\n",
    "        grad_tensor = mlp.compute_grad_log_target(log_target_tensor)\n",
    "        grad_array = grad_tensor.cpu().detach().numpy()\n",
    "        return grad_array\n",
    "\n",
    "    return grad_log_target_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnostic_MCMC(samples):\n",
    "    ess = 0.0\n",
    "    for i in range(samples.shape[0]):\n",
    "        ess += effective_sample_size(samples[i])\n",
    "    ess /= samples.shape[0]\n",
    "    R_hat = gelman_rubin(samples)\n",
    "    return ess, R_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(\n",
    "    sizes,\n",
    "    activations,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    N_steps=110_000,\n",
    "    N_burnin=10_000,\n",
    "    verbose=False,\n",
    "    mh_params=dict(sigma_prop=None),\n",
    "    hmc_params=dict(step_size=0.00004, n_leapfrog=10),\n",
    "    mala_params=dict(step_size=None),\n",
    "    sala_params=dict(step_size=None),\n",
    "):\n",
    "    samples_to_consider = (N_steps - N_burnin) // 10\n",
    "\n",
    "\n",
    "    mlp = Model(sizes, activations)\n",
    "    print(mlp)\n",
    "\n",
    "    log_target_fn = log_target_factory(mlp, X_train, y_train)\n",
    "    grad_log_target_fn = log_target_factory(mlp, X_train, y_train)\n",
    "\n",
    "    N_params = mlp.num_parameters()\n",
    "    mu = np.zeros(N_params)\n",
    "    sigma =  10 * np.ones(N_params)\n",
    "    def prior(size):\n",
    "        return np.random.multivariate_normal(mean=mu, cov=np.diag(sigma), size=size)\n",
    "    \n",
    "    theta_0 = prior(1)[0]\n",
    "    \n",
    "    print(\"_Symmetric_Random_Walk_Metropolis_Hastings_\".center(100).replace(\" \", \"=\").replace(\"_\", \" \"))\n",
    "    MH_sampler = Metropolis_Hastings(\n",
    "        log_target=log_target_fn, theta_0=theta_0, **mh_params\n",
    "    )\n",
    "    sample = np.zeros((5, N_steps-N_burnin+1, N_params))\n",
    "    for i,seed in enumerate(range(40,45)):\n",
    "        np.random.seed(seed)\n",
    "        sample[i], _ = MH_sampler.sample(N_steps, N_burnin, verbose=verbose, return_burn_in=False)\n",
    "        \n",
    "    ess, R_hat = diagnostic_MCMC(sample)\n",
    "    print(f\"\\tEffective sample size: {ess}\")\n",
    "    print(f\"\\tPSRF: {R_hat}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"_Hamiltonian_Monte_Carlo_\".center(100).replace(\" \", \"=\").replace(\"_\", \" \"))\n",
    "    HMC_sampler = HMC(\n",
    "        log_target=log_target_fn, grad_log_target=grad_log_target_fn, theta_0=theta_0, **hmc_params\n",
    "    )\n",
    "    sample = np.zeros((5, N_steps-N_burnin+1, N_params))\n",
    "    for i,seed in enumerate(range(40,45)):\n",
    "        np.random.seed(seed)\n",
    "        sample[i], _ = HMC_sampler.sample(N_steps, N_burnin, verbose=verbose, return_burn_in=False)\n",
    "        \n",
    "    ess, R_hat = diagnostic_MCMC(sample[:,samples_to_consider::, :])\n",
    "    print(f\"\\tEffective sample size: {ess}\")\n",
    "    print(f\"\\tPSRF: {R_hat}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    \n",
    "    print(\"_Adapative_MALA_\".center(100).replace(\" \", \"=\").replace(\"_\", \" \"))\n",
    "    SALA_sampler = AdaptiveMALA(\n",
    "        log_target=log_target_fn, grad_log_target=grad_log_target_fn, theta_0=theta_0, **sala_params\n",
    "    )\n",
    "    sample = np.zeros((5, N_steps-N_burnin+1, N_params))\n",
    "    for i,seed in enumerate(range(40,45)):\n",
    "        np.random.seed(seed)\n",
    "        sample[i], _ = SALA_sampler.sample(N_steps, N_burnin, verbose=verbose, return_burn_in=False)\n",
    "        \n",
    "    ess, R_hat = diagnostic_MCMC(sample)\n",
    "    print(f\"\\tEffective sample size: {ess}\")\n",
    "    print(f\"\\tPSRF: {R_hat}\")\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_noisy_xor_1 = pd.read_csv(\"../data/noisy_xor/data1/training/x.csv\")\n",
    "Y_noisy_xor_1 = pd.read_csv(\"../data/noisy_xor/data1/training/y.csv\")\n",
    "X_noisy_xor_1 = X_noisy_xor_1.to_numpy()\n",
    "Y_noisy_xor_1 = Y_noisy_xor_1.to_numpy(dtype=int).flatten()\n",
    "X_noisy_xor_1_test = pd.read_csv(\"../data/noisy_xor/data1/test/x.csv\")\n",
    "Y_noisy_xor_1_test = pd.read_csv(\"../data/noisy_xor/data1/test/y.csv\")\n",
    "X_noisy_xor_1_test = X_noisy_xor_1_test.to_numpy()\n",
    "Y_noisy_xor_1_test = Y_noisy_xor_1_test.to_numpy(dtype=int).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=2, out_features=2, bias=True)\n",
      "  )\n",
      "  (loss): CrossEntropyLoss()\n",
      ")\n",
      "============================ Symmetric Random Walk Metropolis Hastings =============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110000/110000 [00:29<00:00, 3735.05it/s]\n",
      "100%|██████████| 110000/110000 [00:28<00:00, 3855.25it/s]\n",
      "100%|██████████| 110000/110000 [00:28<00:00, 3827.25it/s]\n",
      "100%|██████████| 110000/110000 [00:29<00:00, 3707.05it/s]\n",
      "100%|██████████| 110000/110000 [00:29<00:00, 3772.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEffective sample size: 240047.67360325475\n",
      "\tPSRF: (10.007678443847217+0j)\n",
      "\n",
      "\n",
      "===================================== Hamiltonian Monte Carlo ======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110000/110000 [03:09<00:00, 581.33it/s]\n",
      "100%|██████████| 110000/110000 [03:08<00:00, 583.12it/s]\n",
      "100%|██████████| 110000/110000 [03:08<00:00, 582.13it/s]\n",
      "100%|██████████| 110000/110000 [03:07<00:00, 585.96it/s]\n",
      "100%|██████████| 110000/110000 [03:06<00:00, 588.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEffective sample size: 304402.21780226333\n",
      "\tPSRF: (5.041939788463842+0j)\n",
      "\n",
      "\n",
      "========================================== Adapative MALA ==========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110000/110000 [00:57<00:00, 1903.86it/s]\n",
      "100%|██████████| 110000/110000 [00:57<00:00, 1905.82it/s]\n",
      "100%|██████████| 110000/110000 [00:57<00:00, 1904.81it/s]\n",
      "100%|██████████| 110000/110000 [00:57<00:00, 1899.94it/s]\n",
      "100%|██████████| 110000/110000 [00:57<00:00, 1900.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEffective sample size: 307765.59371596423\n",
      "\tPSRF: (3.2852045542382093+0j)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run(\n",
    "    sizes=[2, 2, 2],\n",
    "    activations=[nn.ReLU(), nn.ReLU(), None],\n",
    "    X_train=X_noisy_xor_1,\n",
    "    y_train=Y_noisy_xor_1,\n",
    "    N_steps=110_000,\n",
    "    N_burnin=10_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pima_1 = pd.read_csv(\"../data/pima/data1/x.csv\")\n",
    "Y_pima_1 = pd.read_csv(\"../data/pima/data1/y.csv\")\n",
    "X_pima_1 = X_pima_1.to_numpy()\n",
    "Y_pima_1 = Y_pima_1.to_numpy(dtype=int).flatten()\n",
    "X_pima_1, X_pima_1_test, Y_pima_1, Y_pima_1_test = train_test_split(X_pima_1, Y_pima_1, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=8, out_features=2, bias=True)\n",
      "    (1-2): 2 x Linear(in_features=2, out_features=2, bias=True)\n",
      "  )\n",
      "  (loss): CrossEntropyLoss()\n",
      ")\n",
      "============================ Symmetric Random Walk Metropolis Hastings =============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110000/110000 [00:30<00:00, 3584.96it/s]\n",
      "100%|██████████| 110000/110000 [00:30<00:00, 3626.87it/s]\n",
      "100%|██████████| 110000/110000 [00:30<00:00, 3628.68it/s]\n",
      "100%|██████████| 110000/110000 [00:30<00:00, 3624.39it/s]\n",
      "100%|██████████| 110000/110000 [00:30<00:00, 3632.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEffective sample size: 747596.3090287781\n",
      "\tPSRF: (29.288620469770812+0j)\n",
      "\n",
      "\n",
      "===================================== Hamiltonian Monte Carlo ======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110000/110000 [03:17<00:00, 555.60it/s]\n",
      "100%|██████████| 110000/110000 [03:17<00:00, 557.01it/s]\n",
      "100%|██████████| 110000/110000 [03:17<00:00, 556.23it/s]\n",
      "100%|██████████| 110000/110000 [03:17<00:00, 557.23it/s]\n",
      "100%|██████████| 110000/110000 [03:17<00:00, 556.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEffective sample size: 566984.8125997509\n",
      "\tPSRF: (13.048020406290169+0j)\n",
      "\n",
      "\n",
      "========================================== Adapative MALA ==========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110000/110000 [01:01<00:00, 1799.02it/s]\n",
      "100%|██████████| 110000/110000 [01:01<00:00, 1799.39it/s]\n",
      "100%|██████████| 110000/110000 [01:01<00:00, 1792.95it/s]\n",
      "100%|██████████| 110000/110000 [01:01<00:00, 1801.42it/s]\n",
      "100%|██████████| 110000/110000 [01:01<00:00, 1799.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEffective sample size: 627153.9496298257\n",
      "\tPSRF: (9.157149158667828+0j)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run(\n",
    "    sizes=[8, 2, 2, 2],\n",
    "    activations=[nn.ReLU()] * 3 + [None],\n",
    "    X_train=X_pima_1,\n",
    "    y_train=Y_pima_1,\n",
    "    N_steps=110_000,\n",
    "    N_burnin=10_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_penguins = pd.read_csv(\"../data/penguins/x.csv\")\n",
    "X_penguins.drop(columns=\"year\", inplace=True)\n",
    "Y_penguins = pd.read_csv(\"../data/penguins/y.csv\")\n",
    "X_penguins = X_penguins.to_numpy()\n",
    "Y_penguins = Y_penguins.to_numpy(dtype=int).flatten()\n",
    "X_penguins, X_penguins_test, Y_penguins, Y_penguins_test = train_test_split(X_penguins, Y_penguins, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=6, out_features=2, bias=True)\n",
      "    (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (2): Linear(in_features=2, out_features=3, bias=True)\n",
      "  )\n",
      "  (loss): CrossEntropyLoss()\n",
      ")\n",
      "============================ Symmetric Random Walk Metropolis Hastings =============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110000/110000 [00:31<00:00, 3489.48it/s]\n",
      "100%|██████████| 110000/110000 [00:31<00:00, 3523.75it/s]\n",
      "100%|██████████| 110000/110000 [00:31<00:00, 3524.08it/s]\n",
      "100%|██████████| 110000/110000 [00:31<00:00, 3521.58it/s]\n",
      "100%|██████████| 110000/110000 [00:31<00:00, 3501.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEffective sample size: 646651.141004535\n",
      "\tPSRF: (381.141086796736+0j)\n",
      "\n",
      "\n",
      "===================================== Hamiltonian Monte Carlo ======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110000/110000 [03:22<00:00, 543.66it/s]\n",
      "100%|██████████| 110000/110000 [03:22<00:00, 542.75it/s]\n",
      "100%|██████████| 110000/110000 [03:23<00:00, 539.51it/s]\n",
      "100%|██████████| 110000/110000 [03:28<00:00, 527.48it/s]\n",
      "100%|██████████| 110000/110000 [03:25<00:00, 535.47it/s]\n",
      "/Users/benkabongo25/Studies/MVA/S2/Bayesian ML/Project/BayesianML-Project/notebooks/../source/utils.py:42: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ess = ((det_variances / det_cov_matrix)**(1.0 / n_parameters)) * n_samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEffective sample size: nan\n",
      "\tPSRF: (5.699691338830044+0j)\n",
      "\n",
      "\n",
      "========================================== Adapative MALA ==========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110000/110000 [01:02<00:00, 1753.32it/s]\n",
      "100%|██████████| 110000/110000 [01:03<00:00, 1742.42it/s]\n",
      "100%|██████████| 110000/110000 [01:08<00:00, 1594.43it/s]\n",
      "100%|██████████| 110000/110000 [01:02<00:00, 1748.71it/s]\n",
      "100%|██████████| 110000/110000 [01:02<00:00, 1762.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEffective sample size: 3416856.027302026\n",
      "\tPSRF: (2.759558005663408+0j)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run(\n",
    "    sizes=[6, 2, 2, 3],\n",
    "    activations=[nn.ReLU()] * 3 + [None],\n",
    "    X_train=X_penguins,\n",
    "    y_train=Y_penguins,\n",
    "    N_steps=110_000,\n",
    "    N_burnin=10_000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hawks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hawks = pd.read_csv(\"../data/hawks/x.csv\")\n",
    "Y_hawks = pd.read_csv(\"../data/hawks/y.csv\")\n",
    "X_hawks = X_hawks.to_numpy()\n",
    "Y_hawks = Y_hawks.to_numpy(dtype=int).flatten()\n",
    "X_hawks, X_hawks_test, Y_hawks, Y_hawks_test = train_test_split(X_hawks, Y_hawks, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=6, out_features=2, bias=True)\n",
      "    (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (2): Linear(in_features=2, out_features=3, bias=True)\n",
      "  )\n",
      "  (loss): CrossEntropyLoss()\n",
      ")\n",
      "============================ Symmetric Random Walk Metropolis Hastings =============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110000/110000 [00:31<00:00, 3524.01it/s]\n",
      "100%|██████████| 110000/110000 [00:30<00:00, 3554.78it/s]\n",
      "100%|██████████| 110000/110000 [00:30<00:00, 3560.87it/s]\n",
      "100%|██████████| 110000/110000 [00:30<00:00, 3568.05it/s]\n",
      "100%|██████████| 110000/110000 [00:30<00:00, 3566.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEffective sample size: 684980.4230226666\n",
      "\tPSRF: (100.58523962787001+0j)\n",
      "\n",
      "\n",
      "===================================== Hamiltonian Monte Carlo ======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110000/110000 [03:24<00:00, 538.95it/s]\n",
      "100%|██████████| 110000/110000 [03:22<00:00, 542.38it/s]\n",
      "100%|██████████| 110000/110000 [03:23<00:00, 541.14it/s]\n",
      "100%|██████████| 110000/110000 [03:24<00:00, 538.93it/s]\n",
      "100%|██████████| 110000/110000 [03:23<00:00, 539.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEffective sample size: 582743.2611454733\n",
      "\tPSRF: (9.638252912341281+0j)\n",
      "\n",
      "\n",
      "========================================== Adapative MALA ==========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110000/110000 [01:03<00:00, 1736.72it/s]\n",
      "100%|██████████| 110000/110000 [01:03<00:00, 1730.00it/s]\n",
      "100%|██████████| 110000/110000 [01:02<00:00, 1750.24it/s]\n",
      "100%|██████████| 110000/110000 [01:03<00:00, 1739.44it/s]\n",
      "100%|██████████| 110000/110000 [01:03<00:00, 1723.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEffective sample size: 723552.259118391\n",
      "\tPSRF: (7.244244067368107+0j)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run(\n",
    "    sizes=[6, 2, 2, 3],\n",
    "    activations=[nn.ReLU()] * 3 + [None],\n",
    "    X_train=X_penguins,\n",
    "    y_train=Y_penguins,\n",
    "    N_steps=110_000,\n",
    "    N_burnin=10_000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
